{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee877fd-548e-4ef3-b597-29c8ebdc32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.signal import resample_poly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading Functions ---\n",
    "def load_data():\n",
    "    x_seizure = np.load(\"x_filtered_seizures.npy\")\n",
    "    y_seizure = np.load(\"y_filtered_seizures.npy\")\n",
    "    meta_seizure = np.load(\"meta_filtered_seizures.npy\", allow_pickle=True)\n",
    "    x_non = np.load(\"x_1711_nonseizures.npy\")\n",
    "    y_non = np.load(\"y_1711_nonseizures.npy\")\n",
    "    meta_non = np.load(\"meta_1711_nonseizures.npy\", allow_pickle=True)\n",
    "\n",
    "    # Combine datasets\n",
    "    x_all = np.concatenate([x_seizure, x_non])\n",
    "    y_class_all = np.concatenate([y_seizure, y_non])\n",
    "\n",
    "    # Create regression targets (onset, offset)\n",
    "    y_reg_all = np.zeros((len(x_all), 2))\n",
    "    y_reg_all[:len(x_seizure), 0] = meta_seizure[:, 1]  # Onset\n",
    "    y_reg_all[:len(x_seizure), 1] = meta_seizure[:, 2]  # Offset\n",
    "\n",
    "    # Print debug info\n",
    "    print(\"=== Data Summary ===\")\n",
    "    print(f\"Seizure samples: {x_seizure.shape[0]}\")\n",
    "    print(f\"Non-seizure samples: {x_non.shape[0]}\")\n",
    "    print(f\"Total samples: {x_all.shape[0]}\")\n",
    "    print(f\"x_all shape: {x_all.shape}\")\n",
    "    print(f\"y_class_all shape: {y_class_all.shape}\")\n",
    "    print(f\"y_reg_all shape: {y_reg_all.shape}\")\n",
    "    print(\"====================\")\n",
    "\n",
    "    return x_all, y_class_all, y_reg_all\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x_data, y_class, y_reg):\n",
    "        # Store raw data without normalization\n",
    "        self.x_data = x_data\n",
    "        self.y_class = y_class\n",
    "        self.y_reg = y_reg\n",
    "\n",
    "        # Create regression weights (1 for seizure, 0 for non-seizure)\n",
    "        self.reg_weights = np.zeros(len(y_class))\n",
    "        self.reg_weights[y_class == 1] = 1.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
    "        y_class = torch.tensor(self.y_class[idx], dtype=torch.float32)\n",
    "        y_reg = torch.tensor(self.y_reg[idx], dtype=torch.float32)\n",
    "        reg_weight = torch.tensor(self.reg_weights[idx], dtype=torch.float32)\n",
    "\n",
    "        return x, y_class, y_reg, reg_weight\n",
    "\n",
    "# --- Improved Hybrid Conv1D-LSTM Model ---\n",
    "class SeizureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeizureModel, self).__init__()\n",
    "        \n",
    "        # Normalization layer at the start\n",
    "        self.norm = nn.InstanceNorm1d(6, affine=True)\n",
    "        \n",
    "        # Enhanced convolutional layers with residual connections\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(6, 64, kernel_size=25, stride=3, padding=12),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Residual connections\n",
    "        self.residual1 = nn.Conv1d(64, 128, kernel_size=1, stride=2)\n",
    "        self.residual2 = nn.Conv1d(128, 256, kernel_size=1, stride=2)\n",
    "        self.residual3 = nn.Conv1d(256, 512, kernel_size=1, stride=2)\n",
    "        \n",
    "        # LSTM layers with layer normalization\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.4\n",
    "        )\n",
    "        \n",
    "        # Layer normalization after LSTM\n",
    "        self.ln = nn.LayerNorm(512)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Regression head\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply instance normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Convolutional layers with residual connections\n",
    "        x1 = self.conv1(x)\n",
    "        \n",
    "        x2 = self.conv2(x1)\n",
    "        res1 = self.residual1(x1)\n",
    "        x2 = x2 + res1[:, :, :x2.size(2)]  # Match dimensions\n",
    "        \n",
    "        x3 = self.conv3(x2)\n",
    "        res2 = self.residual2(x2)\n",
    "        x3 = x3 + res2[:, :, :x3.size(2)]\n",
    "        \n",
    "        x4 = self.conv4(x3)\n",
    "        res3 = self.residual3(x3)\n",
    "        x4 = x4 + res3[:, :, :x4.size(2)]\n",
    "        \n",
    "        # Prepare for LSTM - swap dimensions\n",
    "        x = x4.permute(0, 2, 1)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_weights = self.attention(x)\n",
    "        x = torch.sum(attn_weights * x, dim=1)\n",
    "        \n",
    "        # Classification output\n",
    "        class_out = self.class_head(x)\n",
    "        \n",
    "        # Regression output\n",
    "        reg_out = self.reg_head(x)\n",
    "        \n",
    "        return class_out, reg_out\n",
    "\n",
    "# --- Training Functions ---\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion_cls, criterion_reg, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    train_history = {'loss': [], 'val_loss': [], 'cls_loss': [], 'val_cls_loss': [],\n",
    "                     'reg_loss': [], 'val_reg_loss': [], 'acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_cls_loss = 0.0\n",
    "        running_reg_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels_cls, labels_reg, reg_weights in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels_cls = labels_cls.to(device).view(-1, 1)\n",
    "            labels_reg = labels_reg.to(device)\n",
    "            reg_weights = reg_weights.to(device).view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_cls, outputs_reg = model(inputs)\n",
    "\n",
    "            # Calculate losses\n",
    "            cls_loss = criterion_cls(outputs_cls, labels_cls)\n",
    "            reg_loss = criterion_reg(outputs_reg, labels_reg) * reg_weights\n",
    "            reg_loss = reg_loss.mean()\n",
    "\n",
    "            loss = cls_loss + 0.5 * reg_loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_cls_loss += cls_loss.item() * inputs.size(0)\n",
    "            running_reg_loss += reg_loss.item() * inputs.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            predicted = (outputs_cls > 0.5).float()\n",
    "            total += labels_cls.size(0)\n",
    "            correct += (predicted == labels_cls).sum().item()\n",
    "\n",
    "        # Training statistics\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_cls_loss = running_cls_loss / len(train_loader.dataset)\n",
    "        epoch_reg_loss = running_reg_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_cls_loss, val_reg_loss, val_acc = evaluate_model(model, val_loader, criterion_cls, criterion_reg)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save history\n",
    "        train_history['loss'].append(epoch_loss)\n",
    "        train_history['val_loss'].append(val_loss)\n",
    "        train_history['cls_loss'].append(epoch_cls_loss)\n",
    "        train_history['val_cls_loss'].append(val_cls_loss)\n",
    "        train_history['reg_loss'].append(epoch_reg_loss)\n",
    "        train_history['val_reg_loss'].append(val_reg_loss)\n",
    "        train_history['acc'].append(epoch_acc)\n",
    "        train_history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Loss: {epoch_loss:.4f}, Cls Loss: {epoch_cls_loss:.4f}, Reg Loss: {epoch_reg_loss:.4f}, Acc: {epoch_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Cls Loss: {val_cls_loss:.4f}, Val Reg Loss: {val_reg_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Best model saved!')\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    print('Final model saved!')\n",
    "\n",
    "    return train_history\n",
    "\n",
    "def evaluate_model(model, loader, criterion_cls, criterion_reg):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_cls_loss = 0.0\n",
    "    running_reg_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_cls, labels_reg, reg_weights in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels_cls = labels_cls.to(device).view(-1, 1)\n",
    "            labels_reg = labels_reg.to(device)\n",
    "            reg_weights = reg_weights.to(device).view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_cls, outputs_reg = model(inputs)\n",
    "\n",
    "            # Calculate losses\n",
    "            cls_loss = criterion_cls(outputs_cls, labels_cls)\n",
    "            reg_loss = criterion_reg(outputs_reg, labels_reg) * reg_weights\n",
    "            reg_loss = reg_loss.mean()\n",
    "\n",
    "            loss = cls_loss + 0.5 * reg_loss\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_cls_loss += cls_loss.item() * inputs.size(0)\n",
    "            running_reg_loss += reg_loss.item() * inputs.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            predicted = (outputs_cls > 0.5).float()\n",
    "            total += labels_cls.size(0)\n",
    "            correct += (predicted == labels_cls).sum().item()\n",
    "\n",
    "    # Calculate metrics\n",
    "    loss = running_loss / len(loader.dataset)\n",
    "    cls_loss = running_cls_loss / len(loader.dataset)\n",
    "    reg_loss = running_reg_loss / len(loader.dataset)\n",
    "    acc = correct / total\n",
    "\n",
    "    return loss, cls_loss, reg_loss, acc\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Overall loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Overall Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Classification loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['cls_loss'], label='Train')\n",
    "    plt.plot(history['val_cls_loss'], label='Validation')\n",
    "    plt.title('Classification Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Regression loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['reg_loss'], label='Train')\n",
    "    plt.plot(history['val_reg_loss'], label='Validation')\n",
    "    plt.title('Regression Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history['acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_regression(y_true, y_pred, title, filename):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([0, 180], [0, 180], 'r--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# --- Main Function ---\n",
    "def main():\n",
    "    # Load data\n",
    "    x_all, y_class_all, y_reg_all = load_data()\n",
    "\n",
    "    # Split data\n",
    "    x_train, x_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
    "        x_all, y_class_all, y_reg_all, test_size=0.2, stratify=y_class_all, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EEGDataset(x_train, y_class_train, y_reg_train)\n",
    "    test_dataset = EEGDataset(x_test, y_class_test, y_reg_test)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(0.85 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = SeizureModel().to(device)\n",
    "\n",
    "    # Print model summary\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Print number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_cls = nn.BCELoss()\n",
    "    criterion_reg = nn.SmoothL1Loss(reduction='none')  # Less sensitive to outliers\n",
    "\n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "    # Train model\n",
    "    history = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        criterion_cls,\n",
    "        criterion_reg,\n",
    "        num_epochs=100\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plot_history(history)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds_cls = []\n",
    "    all_preds_reg = []\n",
    "    all_labels_cls = []\n",
    "    all_labels_reg = []\n",
    "    all_reg_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_cls, labels_reg, reg_weights in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds_cls, preds_reg = model(inputs)\n",
    "\n",
    "            # Collect results\n",
    "            all_preds_cls.append(preds_cls.cpu().numpy())\n",
    "            all_preds_reg.append(preds_reg.cpu().numpy())\n",
    "            all_labels_cls.append(labels_cls.numpy())\n",
    "            all_labels_reg.append(labels_reg.numpy())\n",
    "            all_reg_weights.append(reg_weights.numpy())\n",
    "\n",
    "    # Concatenate results\n",
    "    all_preds_cls = np.concatenate(all_preds_cls).squeeze()\n",
    "    all_preds_reg = np.concatenate(all_preds_reg)\n",
    "    all_labels_cls = np.concatenate(all_labels_cls)\n",
    "    all_labels_reg = np.concatenate(all_labels_reg)\n",
    "    all_reg_weights = np.concatenate(all_reg_weights)\n",
    "\n",
    "    # Classification metrics\n",
    "    preds_cls_binary = (all_preds_cls > 0.5).astype(int)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels_cls, preds_cls_binary))\n",
    "\n",
    "    # Regression metrics (only on seizure samples)\n",
    "    seizure_mask = (all_labels_cls == 1)\n",
    "    if np.any(seizure_mask):\n",
    "        seizure_preds_reg = all_preds_reg[seizure_mask]\n",
    "        seizure_labels_reg = all_labels_reg[seizure_mask]\n",
    "\n",
    "        onset_mae = mean_absolute_error(seizure_labels_reg[:, 0], seizure_preds_reg[:, 0])\n",
    "        offset_mae = mean_absolute_error(seizure_labels_reg[:, 1], seizure_preds_reg[:, 1])\n",
    "\n",
    "        print(\"\\nRegression Performance (Seizure Samples Only):\")\n",
    "        print(f\"Onset MAE: {onset_mae:.2f} seconds\")\n",
    "        print(f\"Offset MAE: {offset_mae:.2f} seconds\")\n",
    "\n",
    "        # Plot regression results\n",
    "        plot_regression(\n",
    "            seizure_labels_reg[:, 0], seizure_preds_reg[:, 0],\n",
    "            'Onset Prediction', 'onset_prediction.png'\n",
    "        )\n",
    "        plot_regression(\n",
    "            seizure_labels_reg[:, 1], seizure_preds_reg[:, 1],\n",
    "            'Offset Prediction', 'offset_prediction.png'\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo seizure samples in test set for regression evaluation\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
